import os
import time
from typing import Any
import torch
import requests
import streamlit as st
from dotenv import find_dotenv, load_dotenv
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from gtts import gTTS

from utils.custom import css_code

load_dotenv()

# Add device detection at the top level
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

def progress_bar(amount_of_time: int) -> Any:
    """
    A very simple progress bar the increases over time,
    then disappears when it reached completion
    :param amount_of_time: time taken
    :return: None
    """
    progress_text = "Please wait, Generative models hard at work"
    my_bar = st.progress(0, text=progress_text)

    for percent_complete in range(amount_of_time):
        time.sleep(0.04)
        my_bar.progress(percent_complete + 1, text=progress_text)
    time.sleep(1)
    my_bar.empty()


def generate_text_from_image(url: str) -> str:
    """
    A function that uses the blip model to generate text from an image.
    :param url: image location
    :return: text: generated text from the image
    """
    image_to_text: Any = pipeline(
        "image-to-text", 
        model="Salesforce/blip-image-captioning-base",
        device=DEVICE
    )

    generated_text: str = image_to_text(url)[0]["generated_text"]

    print(f"IMAGE INPUT: {url}")
    print(f"GENERATED TEXT OUTPUT: {generated_text}")
    return generated_text


def generate_story_from_text(scenario: str) -> str:
    """
    A function using LLAMA-2 1B model to generate a short story.
    :param scenario: generated text from the image
    :return: generated story from the text
    """
    # Initialize the model and tokenizer
    # model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        device_map="auto"
    )
    
    # Create a more structured prompt
    prompt_template = f"""
    Write a complete short story (exactly 50 words) based on this scenario: {scenario}
    Rules:
    - Story must have a beginning, middle, and end
    - Must be exactly 50 words
    - Must be complete (no trailing sentences)
    
    Story:
    """
    
    # Generate the story with adjusted parameters
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=DEVICE,
        max_length=200,  # Increased for complete stories
        num_return_sequences=1,
        temperature=0.7,  # Reduced for more focused generation
        top_p=0.9,
        do_sample=True,
        early_stopping=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
    try:
        with torch.cuda.amp.autocast() if DEVICE == "cuda" else torch.no_grad():
            generated_story = generator(prompt_template)[0]['generated_text']
            # Extract only the story part and clean it
            story = generated_story.split("Story:")[1].strip()
            # Ensure the story ends with proper punctuation
            if not any(story.rstrip().endswith(p) for p in '.!?'):
                story = story.rstrip() + '.'
            # Take only the first 50 words if longer
            words = story.split()
            if len(words) > 50:
                story = ' '.join(words[:50]) + '.'
            return story
    except Exception as e:
        print(f"Error generating story: {e}")
        return "A short story could not be generated. Please try again."


def generate_speech_from_text(message: str) -> None:
    """
    A function using Google Text-to-Speech to convert text to speech
    :param message: short story generated by the model
    :return: None
    """
    try:
        tts = gTTS(text=message, lang='en')
        tts.save("generated_audio.mp3")
        return True
    except Exception as e:
        print(f"Error generating speech: {e}")
        return False


def main() -> None:
    """
    Main function
    :return: None
    """
    st.set_page_config(page_title= "IMAGE TO STORY CONVERTER", page_icon= "üñºÔ∏è")

    st.markdown(css_code, unsafe_allow_html=True)

    st.header("Image-to-Story Converter")
    uploaded_file: Any = st.file_uploader("Please choose a file to upload", type="jpg")

    if uploaded_file is not None:
        print(uploaded_file)
        bytes_data: Any = uploaded_file.getvalue()
        with open(uploaded_file.name, "wb") as file:
            file.write(bytes_data)
        st.image(uploaded_file, caption="Uploaded Image",
                 use_column_width=True)
        progress_bar(100)
        scenario: str = generate_text_from_image(uploaded_file.name)
        story: str = generate_story_from_text(scenario)
        if generate_speech_from_text(story):
            # Change audio file extension from .flac to .mp3
            st.audio("generated_audio.mp3")
        else:
            st.error("Failed to generate audio. Please try again.")

        with st.expander("Generated Image scenario"):
            st.write(scenario)
        with st.expander("Generated short story"):
            st.write(story)


if __name__ == "__main__":
    main()